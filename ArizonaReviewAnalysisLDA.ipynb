{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ReadData as rd\n",
    "# import ReviewAnalysis as ra\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from wordcloud import STOPWORDS, WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "reviewFile=\"dataset/reviewShuffled.json\"\n",
    "businessFile=\"dataset/business.json\"\n",
    "userFile=\"dataset/user.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(alpha,uB,iB,user,item,y_u,y_i,uMap,iMap):\n",
    "    i=uMap[user] if user in uMap else -1\n",
    "    j=iMap[item] if item in iMap else -1\n",
    "    rating=alpha\n",
    "    rating = np.inner(y_u[uMap[i]],y_i[iMap[j]]) + alpha  + (uB[i] if i in uB else 0)  + (iB[j] if j in iB else 0) \n",
    "    return rating  \n",
    "\n",
    "def getPrediction2(alpha,uB,iB,i,j,y_u,y_i,uMap,iMap):\n",
    "    rating = alpha  + (uB[i] if i in uB else 0)  + (iB[j] if j in iB else 0)\n",
    "    if i in uMap and j in iMap:\n",
    "        rating +=np.inner(y_u[uMap[i]],y_i[iMap[j]]) \n",
    "    return rating  \n",
    "\n",
    "def getPrediction3(alpha,uB,iB,i,j,y_u,y_i,y_i2,uMap,iMap):\n",
    "    rating = alpha  + (uB[i] if i in uB else 0)  + (iB[j] if j in iB else 0)\n",
    "    if i in uMap and j in iMap:\n",
    "        rating +=np.inner(y_u[uMap[i]],y_i[iMap[j]]) \n",
    "        rating +=np.inner(y_u[uMap[i]],y_i2[iMap[j]]) \n",
    "    return rating   \n",
    "\n",
    "def getWordCleaner():\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    en_stop = get_stop_words('en')\n",
    "    en_stop=set(en_stop)\n",
    "    en_stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "    p_stemmer = PorterStemmer()\n",
    "    return tokenizer,en_stop,p_stemmer\n",
    "\n",
    "def trainLDA(rData,latentFactor=50,numOfTokens=15000,iterations=40,savePath=\"dataset/lda2_\"):\n",
    "    # latentFactor=50\n",
    "    tokenizer,en_stop,p_stemmer=getWordCleaner()\n",
    "    texts=[]\n",
    "    text=rData['text'].values\n",
    "    nounList=[]\n",
    "    print(\"Start lemmatizing\")\n",
    "    for d in text:    \n",
    "        d = d.lower()\n",
    "        tokens = tokenizer.tokenize(d)\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "#         tagged_text = nltk.pos_tag(stopped_tokens)\n",
    "#         for word, tag in tagged_text:\n",
    "#             if tag in [\"NN\", \"NNS\"]:\n",
    "#                 nounList.append(word)\n",
    "#         stopped_tokens=nounList\n",
    "#     stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        lemm_tokens = [lemm.lemmatize(i) for i in stopped_tokens if(len(i)>2)]\n",
    "        texts.append(lemm_tokens)\n",
    "    print(\"finished lemmatizing\")\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(keep_n=numOfTokens)\n",
    "    corpora.Dictionary.save(dictionary, savePath+str(latentFactor)+\"_factor.dict\")\n",
    "    print(\"Dictionary Created and Saved to: \"+savePath)\n",
    "    corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=latentFactor, id2word = dictionary, passes=iterations)\n",
    "    ldamodel.save(savePath+str(latentFactor)+\"_factor.lda\")\n",
    "    print(\"LDA Model Saved\")\n",
    "    return ldamodel,dictionary\n",
    "\n",
    "def initMaps(tData,vData):\n",
    "    uTrainDict = defaultdict(lambda: defaultdict(int))\n",
    "    iTrainDict = defaultdict(lambda: defaultdict(int))\n",
    "    uValidDict = defaultdict(lambda: defaultdict(int))\n",
    "    iValidDict = defaultdict(lambda: defaultdict(int))\n",
    "    uMap = defaultdict(int)\n",
    "    uCount=0\n",
    "    iMap = defaultdict(int)\n",
    "    iCount=0\n",
    "    for i in tData:\n",
    "        user, item, rating = i['user_id'], i['business_id'], i['stars']\n",
    "        uTrainDict[user][item] = rating\n",
    "        iTrainDict[item][user] = rating\n",
    "        if user not in uMap:\n",
    "            uMap[user]=uCount\n",
    "            uCount+=1\n",
    "        if item not in iMap:\n",
    "            iMap[item]=iCount\n",
    "            iCount+=1\n",
    "    for i in vData:\n",
    "        user, item, rating = i['user_id'], i['business_id'], i['stars']\n",
    "        uValidDict[user][item] = rating\n",
    "    return uTrainDict,iTrainDict,uMap,iMap,uValidDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def learnItemTopics(rData,ldamodel,dictionary,iMap,latentFactor=50):\n",
    "    tokenizer,en_stop,p_stemmer=getWordCleaner()\n",
    "    y_i=np.zeros((len(iMap),latentFactor ))\n",
    "    # print(y_i)\n",
    "    testbCount=0\n",
    "    for b in iMap:\n",
    "        #     print(b)\n",
    "        nounList=[]\n",
    "        for d in rData[rData.business_id==b]['text'].values:\n",
    "            d=d.lower()\n",
    "            tokens = tokenizer.tokenize(d)\n",
    "            stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "#             tagged_text = nltk.pos_tag(stopped_tokens)\n",
    "#             for word, tag in tagged_text:\n",
    "#                 if tag in [\"NN\", \"NNS\"]:\n",
    "#                     nounList.append(word)\n",
    "#             stopped_tokens=nounList\n",
    "            lemm_tokens = [lemm.lemmatize(i) for i in stopped_tokens if(len(i)>2)]\n",
    "            bow = dictionary.doc2bow(lemm_tokens)\n",
    "            topics=ldamodel.get_document_topics(bow)\n",
    "            for topic,prob in topics:\n",
    "                y_i[iMap[b]][topic]=prob\n",
    "        testbCount+=1\n",
    "        if(testbCount%1000==0):\n",
    "            print(testbCount)\n",
    "    return y_i\n",
    "\n",
    "def getTopics(rData,ldamodel,dictionary,iMap,latentFactor=50):\n",
    "    tokenizer,en_stop,p_stemmer=getWordCleaner()\n",
    "    y_i=np.zeros((len(iMap),latentFactor ))\n",
    "    # print(y_i)\n",
    "    testbCount=0\n",
    "    text=\"\"\n",
    "    for b in iMap:\n",
    "        for d in rData[rData.business_id==b]['text'].values:\n",
    "            d=d.lower()\n",
    "            text+=d\n",
    "        testbCount+=1\n",
    "        if(testbCount%1000==0):\n",
    "            print(testbCount)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    lemm_tokens = [lemm.lemmatize(i) for i in stopped_tokens if(len(i)>2)]\n",
    "    bow = dictionary.doc2bow(lemm_tokens)\n",
    "    topics=ldamodel.get_document_topics(bow)\n",
    "    return topics\n",
    "\n",
    "def train_LDA_LFM(lam,tData,vData,factor,trials,rData,ldamodel,dictionary):\n",
    "    print(\"initializing\")\n",
    "    uTrainDict,iTrainDict,uMap,iMap,uValidDict=initMaps(tData,vData)\n",
    "    print(\"initializing done \",len(uTrainDict),len(iTrainDict),len(uMap),len(iMap),len(uValidDict))\n",
    "    uB = defaultdict(float)\n",
    "    iB = defaultdict(float)\n",
    "    latentFactor=factor\n",
    "    y_u=np.random.normal(scale=1./latentFactor,size=(len(uTrainDict),latentFactor ))\n",
    "    print(\"Learning Item Topics \",\" \",factor,\" \",trials)\n",
    "    y_i=learnItemTopics(rData,ldamodel,dictionary,iMap,latentFactor=factor)\n",
    "    print(\"Topic Learnt \",len(y_i),\" \",factor,\" \",trials)\n",
    "    alpha = 0\n",
    "    totalTrials=trials\n",
    "    for counter in range(totalTrials):\n",
    "        alpha=0\n",
    "        for i in uTrainDict:\n",
    "            for j in uTrainDict[i]:\n",
    "                alpha += uTrainDict[i][j] - uB[i] -iB[j] - np.inner(y_u[uMap[i]],y_i[iMap[j]])\n",
    "        alpha /= len(tData)\n",
    "        print(alpha)\n",
    "        for i in uTrainDict:\n",
    "            uB[i] = 0\n",
    "            for j in uTrainDict[i]:\n",
    "                uB[i] += uTrainDict[i][j]  - alpha - iB[j] - np.inner(y_u[uMap[i]],y_i[iMap[j]])\n",
    "            uB[i] /= (lam + len(uTrainDict[i])) \n",
    "        for j in iTrainDict:\n",
    "            iB[j] = 0\n",
    "            for i in iTrainDict[j]:\n",
    "                iB[j] += iTrainDict[j][i]  -alpha - uB[i] - np.inner(y_u[uMap[i]],y_i[iMap[j]])\n",
    "            iB[j] /= (lam + len(iTrainDict[j]))\n",
    "    \n",
    "        for i in uTrainDict:\n",
    "            for lf in range(latentFactor):\n",
    "                y_u[uMap[i]][lf] = 0\n",
    "                for j in uTrainDict[i]:\n",
    "                    y_u[uMap[i]][lf] += y_i[iMap[j]][lf]*(uTrainDict[i][j]  - alpha - iB[j]  +y_i[iMap[j]][lf]*y_i[iMap[j]][lf]-np.inner(y_u[uMap[i]],y_i[iMap[j]]) )\n",
    "                    y_u[uMap[i]][lf]  /= (lam + y_i[iMap[j]][lf]*y_i[iMap[j]][lf])\n",
    "    vMSE = 0\n",
    "    for i in uValidDict:\n",
    "        for j in uValidDict[i]:\n",
    "#             vMSE += ((alpha  + (uB[i] if i in uB else 0)  + (iB[j] if j in iB else 0) - uValidDict[i][j]) **2)\n",
    "            vMSE += ((getPrediction2(alpha,uB,iB,i,j,y_u,y_i,uMap,iMap) - uValidDict[i][j]) **2)\n",
    "    vMSE /= len(vData)\n",
    "    print (vMSE)\n",
    "    return vMSE,alpha,uB,iB,uMap,iMap\n",
    "\n",
    "def train_LDA_LFM2(lam,tData,vData,factor,trials,rData,ldamodel,dictionary):\n",
    "    print(\"initializing\")\n",
    "    uTrainDict,iTrainDict,uMap,iMap,uValidDict=initMaps(tData,vData)\n",
    "    print(\"initializing done \",len(uTrainDict),len(iTrainDict),len(uMap),len(iMap),len(uValidDict))\n",
    "    uB = defaultdict(float)\n",
    "    iB = defaultdict(float)\n",
    "    latentFactor=factor\n",
    "    y_u=np.random.normal(scale=1./latentFactor,size=(len(uTrainDict),latentFactor ))\n",
    "    print(\"Learning Item Topics \",\" \",factor,\" \",trials)\n",
    "    y_i=learnItemTopics(rData,ldamodel,dictionary,iMap,latentFactor=factor)\n",
    "    y_i2=np.random.normal(scale=1./latentFactor,size=(len(iTrainDict),latentFactor ))\n",
    "    print(\"Topic Learnt \",len(y_i),\" \",factor,\" \",trials)\n",
    "    alpha = 0\n",
    "    totalTrials=trials\n",
    "    for counter in range(totalTrials):\n",
    "        alpha=0\n",
    "        for i in uTrainDict:\n",
    "            for j in uTrainDict[i]:\n",
    "                alpha += uTrainDict[i][j] - uB[i] -iB[j] - np.inner(y_u[uMap[i]],y_i[iMap[j]]) - np.inner(y_u[uMap[i]],y_i2[iMap[j]])\n",
    "        alpha /= len(tData)\n",
    "        print(alpha)\n",
    "        for i in uTrainDict:\n",
    "            uB[i] = 0\n",
    "            for j in uTrainDict[i]:\n",
    "                uB[i] += uTrainDict[i][j]  - alpha - iB[j] - np.inner(y_u[uMap[i]],y_i[iMap[j]]) - np.inner(y_u[uMap[i]],y_i2[iMap[j]])\n",
    "            uB[i] /= (lam + len(uTrainDict[i])) \n",
    "        for j in iTrainDict:\n",
    "            iB[j] = 0\n",
    "            for i in iTrainDict[j]:\n",
    "                iB[j] += iTrainDict[j][i]  -alpha - uB[i] - np.inner(y_u[uMap[i]],y_i[iMap[j]]) - np.inner(y_u[uMap[i]],y_i2[iMap[j]])\n",
    "            iB[j] /= (lam + len(iTrainDict[j]))\n",
    "    \n",
    "        for i in uTrainDict:\n",
    "            for lf in range(latentFactor):\n",
    "                y_u[uMap[i]][lf] = 0\n",
    "                for j in uTrainDict[i]:\n",
    "                    y_u[uMap[i]][lf] += (y_i[iMap[j]][lf]+y_i2[iMap[j]][lf])*(uTrainDict[i][j]  - alpha - iB[j]  +y_i[iMap[j]][lf]*y_i[iMap[j]][lf]-np.inner(y_u[uMap[i]],y_i[iMap[j]]) +y_i2[iMap[j]][lf]*y_i2[iMap[j]][lf]-np.inner(y_u[uMap[i]],y_i2[iMap[j]]))\n",
    "                    y_u[uMap[i]][lf]  /= (lam + y_i[iMap[j]][lf]*y_i[iMap[j]][lf] + y_i2[iMap[j]][lf]*y_i2[iMap[j]][lf])\n",
    "    \n",
    "        for j in iTrainDict:\n",
    "            for lf in range(latentFactor):\n",
    "                y_i2[iMap[j]][lf] = 0\n",
    "                for i in iTrainDict[j]:\n",
    "                    y_i2[iMap[j]][lf] += y_u[uMap[i]][lf]*(uTrainDict[i][j]  - alpha - uB[i] - np.inner(y_u[uMap[i]],y_i[iMap[j]]) +y_u[uMap[i]][lf]*y_u[uMap[i]][lf] - np.inner(y_u[uMap[i]],y_i2[iMap[j]]) +y_u[uMap[i]][lf]*y_u[uMap[i]][lf] )\n",
    "                    y_i2[iMap[j]][lf] /= (lam + y_u[uMap[i]][lf]*y_u[uMap[i]][lf])\n",
    "    vMSE = 0\n",
    "    for i in uValidDict:\n",
    "        for j in uValidDict[i]:\n",
    "#             vMSE += ((alpha  + (uB[i] if i in uB else 0)  + (iB[j] if j in iB else 0) - uValidDict[i][j]) **2)\n",
    "            vMSE += ((getPrediction3(alpha,uB,iB,i,j,y_u,y_i,y_i2,uMap,iMap) - uValidDict[i][j]) **2)\n",
    "    vMSE /= len(vData)\n",
    "    print (vMSE)\n",
    "    return vMSE,alpha,uB,iB,uMap,iMap\n",
    "\n",
    "def findLam_Fact(lam,tData,vData,factor,trials):\n",
    "    uTrainDict,iTrainDict,uMap,iMap,uValidDict=initMaps(tData,vData)\n",
    "    uB = defaultdict(float)\n",
    "    iB = defaultdict(float)\n",
    "    latentFactor=factor\n",
    "    y_u=np.random.normal(scale=1./latentFactor,size=(len(uTrainDict),latentFactor ))\n",
    "    y_i=np.random.normal(scale=1./latentFactor,size=(len(iTrainDict),latentFactor ))\n",
    "    alpha = 0\n",
    "    totalTrials=trials\n",
    "    for counter in range(totalTrials):\n",
    "        alpha=0\n",
    "        for i in uTrainDict:\n",
    "            for j in uTrainDict[i]:\n",
    "                alpha += uTrainDict[i][j] - uB[i] -iB[j] - np.inner(y_u[uMap[i]],y_i[iMap[j]])\n",
    "        alpha /= len(tData)\n",
    "        print(alpha)\n",
    "        for i in uTrainDict:\n",
    "            uB[i] = 0\n",
    "            for j in uTrainDict[i]:\n",
    "                uB[i] += uTrainDict[i][j]  - alpha - iB[j] - np.inner(y_u[uMap[i]],y_i[iMap[j]])\n",
    "            uB[i] /= (lam + len(uTrainDict[i])) \n",
    "        for j in iTrainDict:\n",
    "            iB[j] = 0\n",
    "            for i in iTrainDict[j]:\n",
    "                iB[j] += iTrainDict[j][i]  -alpha - uB[i] - np.inner(y_u[uMap[i]],y_i[iMap[j]])\n",
    "            iB[j] /= (lam + len(iTrainDict[j]))\n",
    "    \n",
    "        for i in uTrainDict:\n",
    "            for lf in range(latentFactor):\n",
    "                y_u[uMap[i]][lf] = 0\n",
    "                for j in uTrainDict[i]:\n",
    "                    y_u[uMap[i]][lf] += y_i[iMap[j]][lf]*(uTrainDict[i][j]  - alpha - iB[j]  +y_i[iMap[j]][lf]*y_i[iMap[j]][lf]-np.inner(y_u[uMap[i]],y_i[iMap[j]]) )\n",
    "                    y_u[uMap[i]][lf]  /= (lam + y_i[iMap[j]][lf]*y_i[iMap[j]][lf])\n",
    "    #     print(y_u)\n",
    "        for j in iTrainDict:\n",
    "            for lf in range(latentFactor):\n",
    "                y_i[iMap[j]][lf] = 0\n",
    "                for i in iTrainDict[j]:\n",
    "                    y_i[iMap[j]][lf] += y_u[uMap[i]][lf]*(uTrainDict[i][j]  - alpha - uB[i] - np.inner(y_u[uMap[i]],y_i[iMap[j]]) +y_u[uMap[i]][lf]*y_u[uMap[i]][lf] )\n",
    "                    y_i[iMap[j]][lf] /= (lam + y_u[uMap[i]][lf]*y_u[uMap[i]][lf])\n",
    "    vMSE = 0\n",
    "    for i in uValidDict:\n",
    "        for j in uValidDict[i]:\n",
    "#             vMSE += ((alpha  + (uB[i] if i in uB else 0)  + (iB[j] if j in iB else 0) - uValidDict[i][j]) **2)\n",
    "            vMSE += ((getPrediction2(alpha,uB,iB,i,j,y_u,y_i,uMap,iMap) - uValidDict[i][j]) **2)\n",
    "    vMSE /= len(vData)\n",
    "    print (vMSE)\n",
    "    return vMSE,alpha,uB,iB,uMap,iMap\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Total Data:  156639\n"
     ]
    }
   ],
   "source": [
    "bdata=rd.readData(fileName=businessFile,breakCondition=5000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12634\n"
     ]
    }
   ],
   "source": [
    "categoryDict=defaultdict(int)\n",
    "businessSet=set()\n",
    "for b in bdata:\n",
    "    bid,categoryList,state=b['business_id'],b['categories'],b['state']\n",
    "    if 'Restaurants' in categoryList and state=='ON':\n",
    "        businessSet.add(bid)\n",
    "print(len(businessSet))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10219\n"
     ]
    }
   ],
   "source": [
    "categoryDict=defaultdict(int)\n",
    "businessSet=set()\n",
    "for b in bdata:\n",
    "    bid,categoryList,state=b['business_id'],b['categories'],b['state']\n",
    "    if 'Restaurants' in categoryList and state=='AZ':\n",
    "        businessSet.add(bid)\n",
    "print(len(businessSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Total Data:  4736897\n",
      "837240\n"
     ]
    }
   ],
   "source": [
    "rawdata=rd.readData(fileName=reviewFile,breakCondition=5000000)\n",
    "data=[]\n",
    "for d in rawdata:\n",
    "    bid=d['business_id']\n",
    "    if bid in businessSet:\n",
    "        data.append(d)\n",
    "print(len(data))\n",
    "rawdata=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path=\"dataset/lda2_400000_50_factor.dict\"\n",
    "dictionary = corpora.Dictionary.load(dictionary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldapath=\"dataset/lda2_400000_50_factor.lda\"\n",
    "ldaModel = gensim.models.ldamodel.LdaModel.load(ldapath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latentfactor=50\n",
    "trials=30\n",
    "trainSize=100000\n",
    "tData=data[:trainSize]\n",
    "vData=data[trainSize:trainSize+100000]\n",
    "fileSavePath=\"dataset/lda2_\"+str((trainSize))+\"_\"\n",
    "rData=pd.DataFrame(tData)\n",
    "uTrainDict,iTrainDict,uMap,iMap,uValidDict=initMaps(tData,vData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started:  400000\n",
      "initializing\n",
      "initializing done  152212 10095 152212 10095 97945\n",
      "Learning Item Topics    50   2\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "Topic Learnt  10095   50   2\n",
      "3.73856741363\n",
      "3.71115990778\n",
      "1.58022116302\n",
      "----------lamda: 4.5-----------Trails: 2-------------Factor: 50 MSE: 1.58022116302\n",
      "Best Value for Lamda is:  4.5  vMSE:  1.58022116302\n"
     ]
    }
   ],
   "source": [
    "tData=data[:trainSize]\n",
    "print(\"Training Started: \",len(tData))\n",
    "lamdas=[4.5]\n",
    "trials=[2]\n",
    "factors=[latentfactor]\n",
    "# factors=[1,2,4,6,8,10,15,20,25,30,35,40,45,50]\n",
    "vMSE = np.iinfo(np.int32).max\n",
    "bestLam=0.01\n",
    "bestTrial=1\n",
    "bestFactor=1\n",
    "vMSEList=[]\n",
    "trialList=[]\n",
    "factorList=[]\n",
    "lamdaList=[]\n",
    "for lam in lamdas:\n",
    "    tempvMSE=1\n",
    "    for t in trials:\n",
    "        for f in factors:\n",
    "            tempvMSE,alpha,uB,iB,uMap,iMap=train_LDA_LFM2(lam,tData,vData,f,t,rData,ldaModel,dictionary)\n",
    "#             tempvMSE,alpha,uB,iB,uMap,iMap=findLam_Fact(lam,tData,vData,f,t)\n",
    "#             vMSEList.append(tempvMSE)\n",
    "            print (\"----------lamda: \"+str(lam)+\"-----------Trails: \"+str(t)+\"-------------Factor: \"+str(f)+\" MSE: \"+str(tempvMSE))\n",
    "            if(tempvMSE<vMSE):\n",
    "                vMSE=tempvMSE\n",
    "                bestLam=lam\n",
    "                bestTrial=t\n",
    "                bestFactor=f\n",
    "                bestAlpha=alpha\n",
    "                bestuB=uB\n",
    "                bestiB=iB\n",
    "            vMSEList.append(tempvMSE)\n",
    "            factorList.append(f)\n",
    "#     vMSEList.append(tempvMSE)\n",
    "#     lamdaList.append(i)\n",
    "        \n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter(factorList,vMSEList,color='red',marker='^')\n",
    "# plt.xlabel('Factors')\n",
    "# plt.ylabel('MSE')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Best Value for Lamda is: \",bestLam,\" vMSE: \",vMSE)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started:  400000\n",
      "initializing\n",
      "initializing done  152212 10095 152212 10095 97945\n",
      "Learning Item Topics    50   2\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "Topic Learnt  10095   50   2\n",
      "3.7385810241\n",
      "3.7115155972\n",
      "1.57939785252\n",
      "----------lamda: 4.5-----------Trails: 2-------------Factor: 50 MSE: 1.57939785252\n",
      "Best Value for Lamda is:  4.5  vMSE:  1.57939785252\n"
     ]
    }
   ],
   "source": [
    "tData=data[:trainSize]\n",
    "print(\"Training Started: \",len(tData))\n",
    "lamdas=[4.5]\n",
    "trials=[2]\n",
    "factors=[latentfactor]\n",
    "# factors=[1,2,4,6,8,10,15,20,25,30,35,40,45,50]\n",
    "vMSE = np.iinfo(np.int32).max\n",
    "bestLam=0.01\n",
    "bestTrial=1\n",
    "bestFactor=1\n",
    "vMSEList=[]\n",
    "trialList=[]\n",
    "factorList=[]\n",
    "lamdaList=[]\n",
    "for lam in lamdas:\n",
    "    tempvMSE=1\n",
    "    for t in trials:\n",
    "        for f in factors:\n",
    "            tempvMSE,alpha,uB,iB,uMap,iMap=train_LDA_LFM(lam,tData,vData,f,t,rData,ldaModel,dictionary)\n",
    "#             tempvMSE,alpha,uB,iB,uMap,iMap=findLam_Fact(lam,tData,vData,f,t)\n",
    "#             vMSEList.append(tempvMSE)\n",
    "            print (\"----------lamda: \"+str(lam)+\"-----------Trails: \"+str(t)+\"-------------Factor: \"+str(f)+\" MSE: \"+str(tempvMSE))\n",
    "            if(tempvMSE<vMSE):\n",
    "                vMSE=tempvMSE\n",
    "                bestLam=lam\n",
    "                bestTrial=t\n",
    "                bestFactor=f\n",
    "                bestAlpha=alpha\n",
    "                bestuB=uB\n",
    "                bestiB=iB\n",
    "            vMSEList.append(tempvMSE)\n",
    "            factorList.append(f)\n",
    "#     vMSEList.append(tempvMSE)\n",
    "#     lamdaList.append(i)\n",
    "        \n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter(factorList,vMSEList,color='red',marker='^')\n",
    "# plt.xlabel('Factors')\n",
    "# plt.ylabel('MSE')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Best Value for Lamda is: \",bestLam,\" vMSE: \",vMSE)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  1.255316944411388\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE: \",math.sqrt(vMSE)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started:  400000\n",
      "3.73858965774\n",
      "3.72182301938\n",
      "1.57582063093\n",
      "----------lamda: 4.5-----------Trails: 2-------------Factor: 50 MSE: 1.57582063093\n",
      "Best Value for Lamda is:  4.5  vMSE:  1.57582063093\n"
     ]
    }
   ],
   "source": [
    "tData=data[:trainSize]\n",
    "print(\"Training Started: \",len(tData))\n",
    "lamdas=[4.5]\n",
    "trials=[2]\n",
    "factors=[latentfactor]\n",
    "# factors=[1,2,4,6,8,10,15,20,25,30,35,40,45,50]\n",
    "vMSE = np.iinfo(np.int32).max\n",
    "bestLam=0.01\n",
    "bestTrial=1\n",
    "bestFactor=1\n",
    "vMSEList=[]\n",
    "trialList=[]\n",
    "factorList=[]\n",
    "lamdaList=[]\n",
    "for lam in lamdas:\n",
    "    tempvMSE=1\n",
    "    for t in trials:\n",
    "        for f in factors:\n",
    "            tempvMSE,alpha,uB,iB,uMap,iMap=findLam_Fact(lam,tData,vData,f,t)\n",
    "#             tempvMSE,alpha,uB,iB,uMap,iMap=findLam_Fact(lam,tData,vData,f,t)\n",
    "#             vMSEList.append(tempvMSE)\n",
    "            print (\"----------lamda: \"+str(lam)+\"-----------Trails: \"+str(t)+\"-------------Factor: \"+str(f)+\" MSE: \"+str(tempvMSE))\n",
    "            if(tempvMSE<vMSE):\n",
    "                vMSE=tempvMSE\n",
    "                bestLam=lam\n",
    "                bestTrial=t\n",
    "                bestFactor=f\n",
    "                bestAlpha=alpha\n",
    "                bestuB=uB\n",
    "                bestiB=iB\n",
    "            vMSEList.append(tempvMSE)\n",
    "            factorList.append(f)\n",
    "#     vMSEList.append(tempvMSE)\n",
    "#     lamdaList.append(i)\n",
    "        \n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter(factorList,vMSEList,color='red',marker='^')\n",
    "# plt.xlabel('Factors')\n",
    "# plt.ylabel('MSE')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Best Value for Lamda is: \",bestLam,\" vMSE: \",vMSE)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
